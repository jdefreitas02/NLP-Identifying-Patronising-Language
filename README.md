# NLP - Identifying Patronising Language

This repository contains the codebase for detecting Patronizing and Condescending Language (PCL) from the *Don't Patronize Me* dataset, developed as part of an NLP Shared Task coursework. 

The primary objective is to classify whether a given text sequence contains PCL (Class 1) or does not contain PCL (Class 0), focusing heavily on linguistic subtlety and class imbalance.

## Final Submission Files
* **`BestModel/`**: This directory contains the final compiled model architecture used for the official evaluation.
* **`dev.txt`**: Contains the final binary predictions (0 or 1, one per line) for the official development/validation set.
* **`test.txt`**: Contains the final binary predictions (0 or 1, one per line) for the official blind test set.

---

## The Primary Approach (Best Model)

The final submitted model (`BestModel`) is an ensemble pipeline built upon the **RoBERTa-large** architecture. It was specifically engineered to overcome the two primary challenges of the PCL dataset: extreme class imbalance and high lexical overlap between classes.

**File:** `Roberta_PCM.ipynb`

**Key features of the Best Model pipeline:**
1.  **Task-Adaptive Pre-Training (TAPT):** Prior to classification fine-tuning, the `roberta-large` model underwent unsupervised Masked Language Modeling (MLM) strictly on the PCL dataset. This allowed the embeddings to adapt to the specific sociological domain and terminology of the dataset.
2.  **Focal Loss:** Replaced standard Cross-Entropy Loss to heavily penalize errors on the minority class (PCL) and force the model to focus on hard-to-classify, ambiguous sequences.
3.  **Targeted Regularization:** Implemented a partial layer freeze (bottom 12 layers) and targeted Dropout (20% on the top 12 layers) to prevent catastrophic forgetting and overfitting on the relatively small dataset.
4.  **5-Fold Stratified Ensemble:** The final predictions in `dev.txt` and `test.txt` are generated by ensembling the probability outputs of five distinct models trained via Stratified K-Fold cross-validation, significantly reducing prediction variance.

*Performance:* This model comfortably surpasses the provided `roberta-base` baseline, achieving a highly balanced precision-recall trade-off and an F1 score of 0.607 for the positive class on the dev set.

---

## Experimental Approach: Generative LLM Fine-Tuning

In addition to the primary bidirectional transformer approach, this repository contains an experimental attempt at framing PCL detection as a generative, instruction-following task. 

**File:** `Scalar_Guided_Llama3_Tuning.ipynb`

This notebook explores fine-tuning a quantized large language model (LLM) to perform reasoning-based classification.

**Experimental Methodology:**
* **Base Model:** LLaMA 3 (8B Instruct), quantized to 4-bit via bitsandbytes to fit within standard Colab GPU constraints.
* **Framework:** Unsloth for high-speed training and the `trl` library's `SFTTrainer`.
* **Parameter-Efficient Fine-Tuning (PEFT):** LoRA (Low-Rank Adaptation) was applied to the attention blocks (`q_proj`, `k_proj`, `v_proj`, `o_proj`) and MLP layers with a rank (`r`) of 16.
* **Scalar-Guided Prompting:** Instead of a simple binary classification prompt, the model was instructed using an Alpaca-style template to first output a "Severity Score" (0 to 4 based on the original dataset annotations) before outputting the final "Binary Verdict". The hypothesis was that forcing the model to calculate a scalar severity score first would act as a Chain-of-Thought mechanism, improving binary accuracy.

**Results & Conclusion:**
The LLaMA 3 experiment achieved an F1 score of 0.35 on the positive class, underperforming both the baseline and the RoBERTa ensemble. Analysis indicates that while LLMs excel at generation, applying LoRA to a quantized 8B model for strict, subtle binary classification on a highly imbalanced dataset often yields high variance and low recall compared to purpose-built, fully fine-tuned encoder models. Consequently, the RoBERTa ensemble was selected as the final submission.